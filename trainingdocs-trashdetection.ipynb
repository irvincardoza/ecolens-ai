{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12024865,"sourceType":"datasetVersion","datasetId":7565509}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:02:34.290281Z","iopub.execute_input":"2025-06-01T11:02:34.290534Z","iopub.status.idle":"2025-06-01T11:02:39.730839Z","shell.execute_reply.started":"2025-06-01T11:02:34.290507Z","shell.execute_reply":"2025-06-01T11:02:39.729676Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.0rc1)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\nRequirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport shutil\nimport random\n\n# Read-only input dir\noriginal_data_dir = '/kaggle/input/trash-data/dataset-resized'\n# Writable output dir\noutput_dir = '/kaggle/working/dataset-split'\nsplit_ratio = 0.8\n\n# Split\ncategories = os.listdir(original_data_dir)\nfor category in categories:\n    src = os.path.join(original_data_dir, category)\n    images = os.listdir(src)\n    random.shuffle(images)\n\n    split = int(len(images) * split_ratio)\n    train_imgs = images[:split]\n    val_imgs = images[split:]\n\n    for subset, subset_imgs in zip(['train', 'val'], [train_imgs, val_imgs]):\n        dest = os.path.join(output_dir, subset, category)\n        os.makedirs(dest, exist_ok=True)\n        for img in subset_imgs:\n            shutil.copy(os.path.join(src, img), os.path.join(dest, img))\n\nprint(\"✅ Dataset split recreated successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:00:29.277950Z","iopub.execute_input":"2025-06-01T15:00:29.278242Z","iopub.status.idle":"2025-06-01T15:01:02.801598Z","shell.execute_reply.started":"2025-06-01T15:00:29.278220Z","shell.execute_reply":"2025-06-01T15:01:02.800947Z"}},"outputs":[{"name":"stdout","text":"✅ Dataset split recreated successfully.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\n\nprint(\"✔ Top level:\", os.listdir(\"/kaggle/working\"))\nprint(\"✔ Train folders:\", os.listdir(\"/kaggle/working/dataset-split/train\"))\nprint(\"✔ Val folders:\", os.listdir(\"/kaggle/working/dataset-split/val\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:01:19.890952Z","iopub.execute_input":"2025-06-01T15:01:19.891206Z","iopub.status.idle":"2025-06-01T15:01:19.896635Z","shell.execute_reply.started":"2025-06-01T15:01:19.891187Z","shell.execute_reply":"2025-06-01T15:01:19.896043Z"}},"outputs":[{"name":"stdout","text":"✔ Top level: ['.virtual_documents', 'dataset-split']\n✔ Train folders: ['trash', 'paper', 'cardboard', 'metal', 'glass', 'plastic']\n✔ Val folders: ['trash', 'paper', 'cardboard', 'metal', 'glass', 'plastic']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom PIL import Image\nimport numpy as np\nimport os\n\n# -------------------- SETUP --------------------\nTRAIN_DIR = \"/kaggle/working/dataset-split/train\"\nVAL_DIR = \"/kaggle/working/dataset-split/val\"\ncnn_model_path = \"/kaggle/working/waste_cnn_classifier.h5\"\nmobilenet_model_path = \"/kaggle/working/waste_classifier_mobilenetv2.h5\"\nIMG_SIZE = 224\nBATCH_SIZE = 16\nEPOCHS = 10\nNUM_CLASSES = 6\nclass_names = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n\n# -------------------- DATA LOADERS --------------------\ntrain_gen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.2,\n    horizontal_flip=True\n)\nval_gen = ImageDataGenerator(rescale=1./255)\n\ntrain_ds = train_gen.flow_from_directory(TRAIN_DIR, target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')\nval_ds = val_gen.flow_from_directory(VAL_DIR, target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False)\n\n# -------------------- CNN MODEL --------------------\ncnn_model = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n    MaxPooling2D(2, 2),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D(2, 2),\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D(2, 2),\n    Flatten(),\n    Dropout(0.4),\n    Dense(128, activation='relu'),\n    Dropout(0.3),\n    Dense(NUM_CLASSES, activation='softmax')\n])\ncnn_model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\ncnn_model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[EarlyStopping(patience=3, restore_best_weights=True)])\ncnn_model.save(cnn_model_path)\n\n# -------------------- MOBILENETV2 MODEL --------------------\nbase_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))\nbase_model.trainable = False\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dropout(0.3)(x)\noutput = Dense(NUM_CLASSES, activation='softmax')(x)\nmobilenet_model = Model(inputs=base_model.input, outputs=output)\n\nmobilenet_model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\nmobilenet_model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[EarlyStopping(patience=3, restore_best_weights=True)])\nmobilenet_model.save(mobilenet_model_path)\n\n# -------------------- ENSEMBLE PREDICTION --------------------\ndef get_dustbin_category(waste_type):\n    if waste_type in ['plastic', 'glass', 'metal', 'cardboard', 'paper']:\n        return 'Recyclables ♻️'\n    elif waste_type == 'trash':\n        return 'Landfill 🚮'\n    else:\n        return 'Unknown'\n\ndef ensemble_predict(image_path):\n    img = Image.open(image_path).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n    img_array = img_to_array(img) / 255.0\n    img_array = np.expand_dims(img_array, axis=0)\n\n    cnn_preds = cnn_model.predict(img_array)\n    mobilenet_preds = mobilenet_model.predict(img_array)\n    final_preds = (cnn_preds + mobilenet_preds) / 2.0\n\n    class_idx = np.argmax(final_preds)\n    confidence = round(np.max(final_preds) * 100, 2)\n    waste_type = class_names[class_idx]\n    bin_type = get_dustbin_category(waste_type)\n\n    return waste_type, bin_type, confidence\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:14:38.795416Z","iopub.execute_input":"2025-06-01T15:14:38.796286Z","iopub.status.idle":"2025-06-01T15:22:37.660217Z","shell.execute_reply.started":"2025-06-01T15:14:38.796259Z","shell.execute_reply":"2025-06-01T15:22:37.659618Z"}},"outputs":[{"name":"stdout","text":"Found 2019 images belonging to 6 classes.\nFound 508 images belonging to 6 classes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 210ms/step - accuracy: 0.2378 - loss: 1.7450 - val_accuracy: 0.3858 - val_loss: 1.4692\nEpoch 2/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 177ms/step - accuracy: 0.3860 - loss: 1.5487 - val_accuracy: 0.3957 - val_loss: 1.4176\nEpoch 3/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 177ms/step - accuracy: 0.4063 - loss: 1.4185 - val_accuracy: 0.4291 - val_loss: 1.4063\nEpoch 4/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 179ms/step - accuracy: 0.4678 - loss: 1.3274 - val_accuracy: 0.4882 - val_loss: 1.2831\nEpoch 5/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 176ms/step - accuracy: 0.4776 - loss: 1.2920 - val_accuracy: 0.4902 - val_loss: 1.2290\nEpoch 6/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 179ms/step - accuracy: 0.4929 - loss: 1.2443 - val_accuracy: 0.5472 - val_loss: 1.2081\nEpoch 7/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 175ms/step - accuracy: 0.5371 - loss: 1.1900 - val_accuracy: 0.5650 - val_loss: 1.1364\nEpoch 8/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 174ms/step - accuracy: 0.5595 - loss: 1.1494 - val_accuracy: 0.5768 - val_loss: 1.1232\nEpoch 9/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 176ms/step - accuracy: 0.5527 - loss: 1.1254 - val_accuracy: 0.5650 - val_loss: 1.1220\nEpoch 10/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 176ms/step - accuracy: 0.5584 - loss: 1.1153 - val_accuracy: 0.6240 - val_loss: 1.0636\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEpoch 1/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 246ms/step - accuracy: 0.1578 - loss: 2.1774 - val_accuracy: 0.3543 - val_loss: 1.5869\nEpoch 2/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 175ms/step - accuracy: 0.3484 - loss: 1.6237 - val_accuracy: 0.4961 - val_loss: 1.2860\nEpoch 3/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 175ms/step - accuracy: 0.4962 - loss: 1.2991 - val_accuracy: 0.5886 - val_loss: 1.1065\nEpoch 4/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 174ms/step - accuracy: 0.5345 - loss: 1.1811 - val_accuracy: 0.6260 - val_loss: 0.9958\nEpoch 5/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 177ms/step - accuracy: 0.6131 - loss: 1.0451 - val_accuracy: 0.6594 - val_loss: 0.9194\nEpoch 6/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 173ms/step - accuracy: 0.6161 - loss: 0.9892 - val_accuracy: 0.6791 - val_loss: 0.8573\nEpoch 7/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 175ms/step - accuracy: 0.6801 - loss: 0.8983 - val_accuracy: 0.6850 - val_loss: 0.8170\nEpoch 8/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 172ms/step - accuracy: 0.6686 - loss: 0.8679 - val_accuracy: 0.7067 - val_loss: 0.7803\nEpoch 9/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 172ms/step - accuracy: 0.6865 - loss: 0.8496 - val_accuracy: 0.7087 - val_loss: 0.7529\nEpoch 10/10\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 173ms/step - accuracy: 0.6992 - loss: 0.8019 - val_accuracy: 0.7126 - val_loss: 0.7300\n⚠️ Example image not found. Update the path.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n# -------------------- EXAMPLE --------------------\nexample_image = \"/kaggle/input/trash-data/dataset-resized/cardboard/cardboard10.jpg\"  # Change this path\nif os.path.exists(example_image):\n    waste, bin_type, conf = ensemble_predict(example_image)\n    print(f\"🔍 Waste: {waste} | 🗑️ Bin: {bin_type} | ✅ Confidence: {conf}%\")\nelse:\n    print(\"⚠️ Example image not found. Update the path.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:24:00.530237Z","iopub.execute_input":"2025-06-01T15:24:00.530708Z","iopub.status.idle":"2025-06-01T15:24:00.700076Z","shell.execute_reply.started":"2025-06-01T15:24:00.530686Z","shell.execute_reply":"2025-06-01T15:24:00.699456Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n🔍 Waste: cardboard | 🗑️ Bin: Recyclables ♻️ | ✅ Confidence: 63.53%\n","output_type":"stream"}],"execution_count":12}]}